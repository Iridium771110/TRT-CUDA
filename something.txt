curl -sS https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor | sudo tee /etc/apt/keyrings/microsoft.gpg
echo "deb [arch=amd64 signed-by=/etc/apt/keyrings/microsoft.gpg] https://packages.microsoft.com/repos/vscode stable main" | sudo tee /etc/apt/sources.list.d/microsoft.list

有些情况下release模式会产生莫名其妙的错误，往往来自于一些不起眼的warning，在激进优化模式下出现了离奇的问题，常见于不明原因的死循环、段错误等等。
因而至少应当在编译选项中添加 如
set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Werror=return-type")
set(CMAKE_CUDA_FLAGS "-Xcompiler -Werror=return-type -fPIC ${CMAKE_CUDA_FLAGS}\
                    -gencode arch=compute_89,code=sm_89 -std=c++17")

TRT-Plugin things
插件形式的编译中，可能出现CMAKE中某些LINK库因为未被扫描到需求而未发生链接（ldd查看无目标链接库）
此时应当使用 target_link_options(name PUBLIC "LINKER:-no-as-needed"),使指定库强行链接
猜测这个情况可能是因为注册机制导致的问题，在使用 CMAKE 编译依赖 libtorch的库时也发生了这个现象
  由于注册机制可以在不包含子类实现及头文件的情况下，仅使用父类头文件及实现库完成编译工作，可能因此导致运行时找不到子类实现但是编译及链接时没有问题（ldd也未出现 undefined symbol）的情况
  该原因仅为猜测

Plugin在各个阶段的运行顺序，针对orin trt10.5.0
buildSerialize：
  construct->suppor-check->clone->config->init->config->enqueue->terminate->destroy
  需要注意从clone开始，执行的是clone后的对象
  log中有很多重复的东西，可能内部有递归
  因为这里对clone对象进行了init，因此不会有下面infer时的init问题
  
inference:
  construct->init->clone->config->enqueue
  需要注意，此时config开始，执行的对象是clone后的那个plugin
  全部执行完毕，本engine需要释放时，clone对象destroy->原始对象terminate->原始对象destroy
  因此如果clone时调用的构造有部分操作在init内，那么可能就会导致clone的和原始对象不同，发生未初始化带来的错误

和网上部分说法可能不同的是，似乎initialize和teminate在调用内部有自关联，可能存在回调情况
configurePlugin方法可以用作部分运行时检查的手段
support-format-combination这里可以用于engine内部中间张量（本plugin的输入输出等）格式、类型等的指定，这个指定可以限定engine build过程中对计算方案的选择范围，不让格式乱飘
部分地方提示，检查设置时可能必须自小向大，不应该以比当前大的索引作为基准来检查自身，而应该以比当前小的索引作为基准检查
部分可参考文档 https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#extending
Tensor-setAllowedFormat 对内部中间张量不起作用，估计仅对engine输入输出进行设置时起作用
builder = createInferBuilder() 似乎会存在一定的内存泄漏（存疑，数量还不小，cpu内存），delete builder之后内存没有相应减小
plugin选择数据类型应该由输入输出描述子来直接获取而不是一个op attr

高版本如 10.5 存在对attention的图优化算子融合等操作，实际用于Superglue时发现cross-attention部分会造成大比例精度问题（并且部分帧结果不稳定，在两三种结果间跳变）怀疑存在bug
涉及norm类型的layer时，half的精度可能不够，在SuperPoint中发生build-engine时若打开fp16，有时norm部分（pow、sum等节点）全部会选择half，会导致严重的精度问题（输出0、nan等），选择float时结果正确
build-engine应当注意对格式的指定，防止部分莫名其妙错误

注意一些预编译指令的配套使用，可能会发生奇怪位置的段错误
比如 #pragma pack(x) <--> #pragma pack()应配对使用

要分析 DLA，请在使用 NVIDIA Nsight Systems CLI 时添加 --accelerator-trace nvmedia 标志

tensorrt和onnx 对于卷积的权重排布方式为 G-co-ci-kkk
MatMul 的输入当是一个常量参数时，排布方式为习惯性的矩阵排列（无特殊变换的正常矩阵顺序）
Gemm 的乘加常数矩阵的排列方式也是习惯性正常矩阵顺序

关于部分源文件的下载（BSP）
https://developer.nvidia.com/embedded/jetson-linux-r3541

在矢量操作、tensorcore等操作中，尤其是当指令操作为长字段，但是操作数仅能给首寄存器的时候（注意SASS操作数和PTX操作数之间存在一定的区别，PTX更灵活，寄存器不需要连续，可以编译时重排列），数据存储时的寄存器顺序应当连续，非连续寄存器会导致寄存器间数据移动，产生大量不必要开销（显著的IMAD，MOV类型指令，在大kernel占用率不高的情况下，产生显著的 no instruction stall）
LDGDEPBAR 该barrier和 cp.async.commit_group 似乎是强绑定，但是对延迟的影响待定
block内共享内存超出48k需要使用动态共享内存设置
共享内存进行矢量操作时，似乎会出现随机冲突（指分次采样结果均不同），尤其是16字节操作时。但是进行4字节操作时不会出现冲突，即使bank已经对齐，该问题也可能由于首地址错位/错误而产生，需要经过理论位置排查
8threads 同一全局地址的boardcast会导致 uncoalse L2 和 uncoalse shared，类似于冲突和全局不对齐的行为，因此需要考虑将其均匀分布，减少同一全局位置的广播（在LDGSTS中证实存在这一现象）
异步拷贝可与计算指令执行显式异步（异步指令如LDGSTS，可以显示使用ptx等方式，可以灵活控制，由用户指定），部分同步指令，如ldmatrix与计算指令可以隐式异步（由gpu硬件执行自动指令并行，不可控）。共享内存的异步载入对延迟隐藏的效果较少，而全局内存的异步载入对延迟隐藏有显著的作用，尤其是在载入的循环量较大的时候，此效果更为明显。流水作业可以考虑2-3级
在某些情况下，即使寄存器连续，进行矢量存取时，如果采用cpp高级语言形式操作，可能会出现断裂情况，导致内存效率低下，尤其是在针对全局内存的写操作（这依赖于编译器的编译结果，尚未找到合适的提示手段）。而采用ptx st.global.v4.u32时则不会出现该问题。
ampere指令并行（ILP）依赖于编译器和硬件，触发条件似乎比较玄，当触发时，对于不同单元（如计算和内存，在矩阵乘法中，对ldsm和mma）的执行延迟有良好的掩盖作用，一定程度上可以弥补因占用率不足带来的延迟问题
syncthread 之后进行 ldgsts时，会前置出现3*LDS RZ，此现象和sync的数目强绑定（但是trt中的source显示并没有，不知有什么神奇的操作）
一般f16的结果和误差应当在千分位上，但是最大个体误差可能存在百分位与十分位上
L1与shared处于同一位置，理论上共享同一资源空间，因此行为可能一致。在LDGSTS时ncu报告了非shared指令源头的bank conflict
按照往常经验，LDGSTS自global读取时的内存排布也应遵循类似的bank分离形式，典型的应当采取线程连续读取，随机乱读可能会产生conflict，但是具体形式待实际查验，一个已验证的现象是，当线程读取完全连续时，并且每行长度为128byte整数倍（8T-16B）时不会产生conflict，若非128byte整数倍（8T-16B）则会产生conflict，更多的，即使行长为128byte整数倍，但是所读并非128byte导致分行，也会产生conflict。当前可行的情况仅前述的第一个现象，看起来其冲突原理与shared类似，但是要求似乎会更加严格，更容易产生。这一conflict在统计中会以L1 uncoalesced wavefront的形式出现，而在shared conflict统计中只需要满足连续非交错（可能存在一些异步时的冲突操作？）
实际操作中，发现cp.async在ampere中，当采用循环方式调用的时候，如果循环数未直接指定，则需要在该循环组commit之后进行wait 0,否则即使在后续的异步拷贝组后进行了wait 0,改组似乎也会出现未同步完成拷贝的情况（该现象出现在mha调试中，导致了内部矩阵乘法中首个轮次的结果错误，在采用指定循环数值或者在下一组进行前任意位置进行wait后，结果正确 -->完蛋，检查了一下没准是个bug，因为间接指定时的计算会导致越界读写，没有紧随wait的情况下可能出现拷贝覆盖？以后遇到应该再检查一下，暂时定义为越界bug）
对于初始化等操作，当使用低字节精度时（因为一个reg长度是4字节），若非特别指定的类型（如half2,short2），则一个寄存器只会存一个数值，在SASS中会出现大量的PRMT操作，可在初始化时显式使用int类型指针代替，可显著减少指令数（PRMT），但CS2R会有增加，整体是有利的（在mha调整中被证实），副作用似乎会引起寄存器数增加（？）
cp.async.cg 单元长度只能是16字节，.ca可以是4,8字节
grid的x/y/z的顺序，以及数值调配上，似乎会对L2命中率产生极大的影响，而这一命中率对于流水线操作、降低访存延迟等似乎有重要作用（在调试qkv-extract-from-posembed中，在orin上被证实的现象），这可能同时需要在kernel内及启动参数上对线程任务、束任务、块任务的划分上进行调整，与Nv的缓存策略有关，比较玄学，可以考虑尝试并总结经验，当前认为主要关注点应当是grid的调整，并且关注由之产生的内存事务分配，亦可能与SM上驻留的blocks它们的任务范围是否连续合理有关。
根据现象判断，异步流水可以隐藏延迟，但是同时多warp驻留也可以隐藏延迟，当warp足够时仅通过切换就可以达到类似的效果，因此有时候异步流水的作用并不能体现。

L1 与 shared可能因为同时访问，造成二者之间的bank conflict，因此LDGSTS操作中，如果L1并未对齐缓存行的话，极可能出现冲突现象，（按照ncu统计来看，这个现象不一定发生在命令所在地方，但是在source中的 L1 wavefront shared excessive 会有直接体现。在统计table中，其数目可能会呈现一定的随机性，但并不大幅变动，大幅变动会随着代码、硬件架构而发生。
L1 的内存粒度为 128字节的缓存行（access颗粒度可能是32字节），一个时钟周期操作一个缓存行->大致可以推算其带宽
L2 带宽可以看ncu 吞吐量 和效能百分比大致推算

关于Blackwell
根据手册存在TPC（两个SM组成的协作组，可以互相访问私有存储空间，据称能有效降低访存压力），TMA等专用设备线的开设，看起来用途很不错。
关于cuda13-trt10.13+
int8量化依旧可以使用隐式量化（setdynamicrange），但是fp8级别的量化需要采用显式量化的形式才能识别
权重的量化形式的输入需要经过QDQ，然后在conv中以setinput input1的形式输入，Q节点应当指定outputtype为FP8,DQ节点应当与conv节点数据类型一致（包括bias，bias不必经过量化节点）
在per-channel量化时，QDQ节点需要指定轴 setaxis否则会报错
识别一个quantconv需要input路加QDQ，weight路加QDQ，输出不必QDQ，即五个节点会被识别为quantconv
显式量化在实际操作中有可能造成计算图的分裂？目前观察到e4m3后面会多出现一个cast函数。并且有的conv在e4m3上的速度还不如f16，也存在明显的local mem操作。
若设置networkdefinitioncreationflag::kSTRONGLY_TYPED，此时表示各节点将会严格按照节点type设置，--fp16等宽泛精度设置将会变成冲突项（报错or失效）
实测不设置上述选项也可以进行FP8量化
kernel-fusion的水平更进一步的提升
从kernel命名上看，mha的算法形式应当和前作系列一致（10.3），但是在thor上由于硬件的提升而获得了性能上长足的进步（或许有大部分是硬件相关的软件指令升级？）

kernel，TC，Cuda的手写情况，特点及体感需要实际尝试后总结，看起来会有很多新特点，有更高的操作空间。目测TMA存在和之前ldg类似的swizzle操作，但是可能有不同的限制和通信、唤起和同步需求

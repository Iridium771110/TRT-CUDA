关于网络训练反传的一些粗鄙思想
基本都是梯度反传的一套，那么和线性优化或者随机梯度下降的ods是否就是同一回事呢？不过这更像是一个系统优化以逼近最优解的过程，数值方法求解函数？


关于网络设计单元的一些粗鄙思想
cnn = conv， conv从数学意义上可以理解为 filter
输入如果作为一个信号来看，即在使用过滤器进行滤波，从时序输入上来讲似乎更容易理解
trans的核心attn在于矩阵相乘，拆开看做 矩阵矢量内积的组合。内积视为投影，可以表征各矢量在本矢量上的相似性（参考余弦相似），因此这个相乘是在找特征矢的相似性（？）
但是操作上似乎是没有单位矢的做法在内，可能这个东西会让模型自行训练吧，也许也是trans难以训练的原因之一？
根号d的除法感觉只是一个工程trick，防止某些异常值溢出或者反传失效之类的事情
看起来soft的目的在于归一化并且同时锐化差距，使最大值更加明显
第二个attn暂时不太懂，为啥要有两次乘法呢

既然内积可以用，那么外积是否也可用（不过外积好像很麻烦来着……）

关于大模型推理的一些理解
自回归的含义即自动拿前面的输出组合为后面的输入进行循环推理。decoder应当每个大模型只有一个。但是自回归的推理中在进行循环单token生成，操作中也可以进行多token
prefill 即从输入token中进行attn获得kv cache并得出第一个输出token的过程，一般为计算密集型
decode 从第一个输出，进行顺序串行循环推理解码，每一次依赖前一次获得的token，对其提取qkv之后加入kv cache，然后进行attn，最后获取一个输出token，循环直到到达停止条件
上述的attn过程应该有很多轮的trans过程（不太清楚）
因此在decode中，串行循环时会产生大量的gemv，成为访存密集型，从而拖慢整体效率
投机采样的原理类似于蒸馏后的小模型与大模型的配合，小模型输出连续多个预测，大模型对其进行评测，出于推理结果一致性的考量，最后保留的输出是具有一致性的连续前缀。在这个情况下大模型的推理会变成gemm为主，因而也有“一次推理输出一个token与多个token的耗时相近”的说法

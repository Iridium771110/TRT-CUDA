关于网络训练反传的一些粗鄙思想
基本都是梯度反传的一套，那么和线性优化或者随机梯度下降的ods是否就是同一回事呢？不过这更像是一个系统优化以逼近最优解的过程，数值方法求解函数？


关于网络设计单元的一些粗鄙思想
cnn = conv， conv从数学意义上可以理解为 filter
输入如果作为一个信号来看，即在使用过滤器进行滤波，从时序输入上来讲似乎更容易理解
trans的核心attn在于矩阵相乘，拆开看做 矩阵矢量内积的组合。内积视为投影，可以表征各矢量在本矢量上的相似性（参考余弦相似），因此这个相乘是在找特征矢的相似性（？）
但是操作上似乎是没有单位矢的做法在内，可能这个东西会让模型自行训练吧，也许也是trans难以训练的原因之一？
根号d的除法感觉只是一个工程trick，防止某些异常值溢出或者反传失效之类的事情
看起来soft的目的在于归一化并且同时锐化差距，使最大值更加明显
第二个attn暂时不太懂，为啥要有两次乘法呢 == 有点像soft以后表征为一系列的概率，和V的相乘结果就是一个概率加权后的特征和结果

既然内积可以用，那么外积是否也可用（不过外积好像很麻烦来着……）

关于大模型推理的一些理解
自回归的含义即自动拿前面的输出组合为后面的输入进行循环推理。decoder应当每个大模型只有一个。但是自回归的推理中在进行循环单token生成，操作中也可以进行多token
prefill 即从输入token中进行attn获得kv cache并得出第一个输出token的过程，一般为计算密集型
decode 从第一个输出，进行顺序串行循环推理解码，每一次依赖前一次获得的token，对其提取qkv之后加入kv cache，然后进行attn，最后获取一个输出token，循环直到到达停止条件
上述的attn过程应该有很多轮的trans过程（不太清楚）
因此在decode中，串行循环时会产生大量的gemv，成为访存密集型，从而拖慢整体效率 == 本质上kv cache剔除了冗余计算，提升了推理效率。但是从高性能角度来看，每次添加的一个token带来的新的KV以及对新增量做的新量计算，这一部分存在大量的gemv，导致了mem bound。
可以参考 https://www.cnblogs.com/rossiXYZ/p/18799503 里面的冗余分析这一步。至少从形式上，kv cache引发了 QiKT 和 soft()V 两组gemv，前置的QKV增量式提取也引起了三组gemv
投机采样的原理类似于蒸馏后的小模型与大模型的配合，小模型输出连续多个预测，大模型对其进行评测，出于推理结果一致性的考量，最后保留的输出是具有一致性的连续前缀。
在这个情况下大模型的推理会变成gemm为主，因而也有“一次推理输出一个token与多个token的耗时相近”的说法(?此说法感觉存疑，也有按照batch解释的版本，本质上都是利用冗余算力，或者减少顺序推理中的权重和缓存加载次数提升效率，（对于这个的理解应当向专业的人咨询一下），如果仅仅是利用冗余算力，那么在边际场景中其实价值并不大，因为本质上并没有减少计算量)
百度panndle应该实现了一种拼接形式的方案，应该是理解中的可以形成gemm的方案，它应该有计算量的变化，会比传统的多batch有更高的效率--从计算逻辑上想这是应该存在可行方案的，但是需要找到实际实现方案
另外从原理上直观感觉，投机推理的形式是增大了计算量的，并且其预测部分的增量在两个模型大小和性能差距不大的情况下，会形成很大的预测耗时，变成拖慢的大头
感觉本质上投机解码的提升重点在于算法层面如何提升草稿的接收率上，并且会有一个边际值，因为一般更高的接收率意味着更大的预测耗时（称overhead）

另外有一种说法，这些都是概率模型，
